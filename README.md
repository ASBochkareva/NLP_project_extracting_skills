# NLP-проект: Извлечение навыков из описаний вакансий

## Аннотация
Данная работа представляет NLP-систему для автоматического извлечения
профессиональных навыков из текстов вакансий на русском языке.
Исследование включает в себя:
-   Гибридный подход, сочетающий дообученную языковую модель
    (RuBERT-tiny), синтаксический анализ (DeepPavlov + spaCy) и линейные
    комбинации признаков
-   Методику обработки зашумленных текстов вакансий с эффективным
    объединением составных терминов (например, "управление_проектами")
-   Специализированный словарь из 1,000 навыков, автоматически
    извлеченных из данных HeadHunter

Эксперименты показали качество модели F1=0.749. Система предназначена
для автоматизации анализа требований рынка труда, подбора персонала и
разработки образовательных программ.

## Цель проекта
Выделение навыков из описаний вакансий как именованных сущностей. Проект
направлен на автоматизацию процесса анализа текстовых данных с целью
извлечения ключевых навыков, необходимых для выполнения определенной
работы.

## Применение результатов проекта
Применяя результаты проекта, можно решать следующие задачи:
-   Анализ рынка труда: определение наиболее востребованных навыков и
    тенденций на рынке труда.
-   Подбор персонала: автоматизация процессов найма на основе навыков
    кандидатов.
-   Образование и обучение: разработка образовательных программ на
    основе актуальных потребностей рынка труда.

## Проблематика
Исследование показало, что данные о навыках заполнены не для всех
вакансий. А для тех, вакансий, где навыки выделены, зачастую можно
наблюдать их сильную зашумленность общехарактерными навыками,
актуальными для всех ролей, и не отражающих какую-то специфику. Такими,
как: ответственность, работа в команде, целеустремленность, пользование
ПК, MS Office и т.д. В подобных описаниях навыков профессиональная роль
размывается, теряет свою четкую идентификацию.

Следовательно, наибольший интерес представляет извлечение навыков
непосредственно из текстовых описаний вакансий. Однако, при выполнении
проекта необходимо будет учитывать низкое качество данных. Порою,
вакансии составлены неграмотно или не полностью. Грамматические и
пунктуационные ошибки могут усложнять анализ текстов. Тексты с описанием
вакансий бывают зашумлены специфичными печатными символами, целью
использования которых является как привлечение визуального внимания
соискателя, так и усиление эмоциональной окраски вакансии. Обилие
повторений и общих выражений, особенно характерных для вакансий массовых
специальностей, когда описание множества вакансий почти полностью
идентично. Все это снижает пригодность информации для
автоматизированного анализа.

## Ход выполнения проекта
1.  **EDA**, подготовка текстовых данных.
2.  **Дообучение RuBERT-tiny на MLM** (Masked Language Modeling). MLM
    - это метод обучения моделей, при котором модель обучается
    предсказывать пропущенные слова в тексте. Для обучения будет
    использоваться модель Rubert-tiny - это уменьшенная версия модели
    BERT, адаптированная для русского языка. Дообучение модели на MLM
    позволит ей лучше понимать контекст и структуру текста, что важно
    для последующего извлечения навыков.
3.  **Синтаксический анализ с deeppavlov_syntax_parser.** Синтаксический
    анализ поможет лучше понять структуру предложений и выделить
    ключевые слова, которые могут быть связаны с навыками
4.  **Извлечение навыков через линейные комбинации признаков.**

## Данные 
Исходный датасет содержал 147 000 русскоязычных описаний вакансий,
собранных с сайта hh.ru за период с марта по декабрь 2024, относящихся к
отрасли торговли.

## Этап 1: Предварительный анализ данных (EDA)
1.  Предварительная подготовка корпуса данных (удаление лишних символов,
    дубликатов, анализ длины текста по количеству символов и слов,
    удаление похожих пар текстов). В результате подготовлен датасет с
    49401 уникальных описаниями требований к соискателям, который будет
    использоваться для обработки на следующем этапе 2.
2.  Выполнен анализ навыков из столбца key_skills. В итоге получен
    датасет с 1000 самых популярных навыков, которые выделяют сами
    рекрутеры при размещении вакансии, для применения на этапе 3.

## Этап 2: Дообучение RuBERT-tiny на MLM
1.  Составлен дополнительный словарь профессиональных терминов для
    расширения словаря токенизатора. Т.к. ранее при анализе текста с
    описанием вакансии встретились специализированные термины, состоящие
    из цифр и букв (вроде b2b, b2c, 1c, bitrix24), которые важно не
    разбивать при токенизации. Эти термины часто используются в текстах
    вакансий и содержат значимую информацию о необходимых навыках
    сотрудника. Включив их в словарь, в дальнейшем мы обеспечим их
    корректную обработку при токенизации, без дробления на части.
2.  Токенизация текста. Для токенизации создан class
    RussianJobTokenizer, основанный на AutoTokenizer, который
    -   добавляет специальные токены, чтобы модель корректно
        обрабатывала профессиональные термины и аббревиатуры (выявленные
        на предыдущем шаге)
    -   выполняет базовую обработку текста (токенизацию) с добавлением
        специальных маркеров начала и конца предложения.
    Длина входного текста ограничена 510 токенами (чтобы избежать
    превышения максимального размера последовательности в модели BERT).
3.  Для выполнения задачи Masked Language Modeling (MLM) - предсказания
    пропущенных слов в тексте, загружаем предобученную языковую модель
    rubert-tiny.
4.  Подготавливаем наш датасет для обработки языковой моделью.
    Преобразовали исходный pandas-датафрейм в формат HuggineFace
    Dataset. Дополнительно проанализировали длину токенизированных
    текстов, чтобы выявить возможные аномалии (слишком короткие тексты
    длиной менее 10 токенов).
5.  Выполняем настройку обучения (TrainingArguments, DataCollator
    добавляет паддинг и маскирование 15% токенов) Гиперпараметры:
    -   Размер батча: 8.
    -   Количество эпох: 5.
    -   Скорость обучения: 3e-5.
    -   Оптимизатор: AdamW.
    Был использован колларатор данных, который применял маску к 15%
    токенов в каждой последовательности, и задавал правило дополнения
    пробелами (padding) для выравнивания размеров всех вводимых данных.

По завершении обучения была проведена оценка качества модели с помощью
метрики **Perplexity: 8.39**, что показывает способность модели
правильно восстанавливать скрытые элементы текста.

Результат: **модель RuBERT-tiny дообучена MLM на основе корпуса
русскоязычных текстов с описанием требований к навыкам** соискателям
вакансий из отрасли торговли.
В дальнейшем дообученная модель используется для генерации эмбеддингов
на этапе 3

## Этап 3: Синтаксический анализ с DeepPavlov_syntax_parser и spaCy 
Реализован класс EnhancedSyntaxAnalyzer, который выполняет:
1.  Генерацию эмбеддингов с дообученной на предыдущем этапе Rubert-Tiny,
    для преобразования текста в векторные представления, сохраняющие
    семантику и контекст профессиональных терминов
    -   выполняется предобработка входного текста (очистка, удаление
        стоп-слов, нормализация)
    -   токенизация: получение векторных представлений с сохранением
        позиций слов. Используется `mlm_tokenizer`, текст разбивается на
        субтокены BERT по позициям (start, end), для каждого субтокена
        генерируется эмбеддинг (get_token_embeddings)
    -   выполняется группировка субтокенов в слова
        (get_token_embeddings)
2.  Синтаксический анализ с DeepPavlov и spaCy для выявления структурных
    связей между словами для точного выделения навыков
    -   Получение грамматических зависимостей с помощью
        syntax_ru_syntagrus_bert. На выходе сформированы синтаксические
        деревья в формате CoNLL-U (enhanced_parse_with_features). Формат
        вывода CoNLL-U содержит: ID токена, словоформу, лемму, POS-тег,
        морфологические признаки, ID головного слова, тип синтаксической
        связи.
    -   Лемматизация (в дальнейшем буду использовать для поиска
        синонимов) через pymorphy2
    -   простановка POS-тегов и зависимостей выполняется с помощью spaCy
    -   Объединение данных (DeepPavlov и spaCy), сопоставление
        результатов DeepPavlov и spaCy по позициям слов в тексте
    -   Расчет глубины зависимостей (расчитываются уровни вложенности до
        корня дерева (где head = 0))
    Особенности реализации: Приоритет POS-тегов от spaCy (как более
    точных для русского), Грамматические связи (deprel) берутся из
    DeepPavlov (более детализированные), Позиции слов (start, end)
    сохраняются для сопоставления с эмбеддингами Для каждого токена
    формируется структура, содержащая:
    -   Синтаксическую роль (nsubj, obj и др.)
    -   Связь с головным словом
    -   Нормализованную форму (лемму)
    -   Признаки составных терминов
    -   Векторное представление слова
3.  Объединение составных терминов (например, 'управление проектами')
    для автоматического выделения сложных навыков
    -   На этапе 1.EDA был подготовлен словарь самых популярных навыков
    -   Словарь обработан, выделены базовые навыки base_skills и
        составные навыки compound_skills
    -   Найденные в тексте составные навыки объединяются в одно слово с
        помощью нижнего подчеркивания
    -   Выполняется пересчет эмбеддинга
4.  Разметка профессиональных навыков
    -   Если термин есть в base_skills или является составным
        (compound_skills) устанавливается POS-тег 'PROSKILL'
    -   Важно: тег ставится только если термин есть в словаре
5.  Извлечение признаков для классификации навыков. Чтобы учесть
    смысловую близость слов через их векторные представления извлекаются
    признаки:
    -   семантические, чтобы учесть смысловую близость слов через их
        векторные представления. Извлекаются нормализованные эмбеддинги,
        признаки для классификатора
    -   синтаксические, чтобы учесть структурную роль слова в
        предложении. Извлекается бинарные признак is_root является ли
        слово корнем синтаксического дерева, Число шагов от слова до
        корня дерева dependency_depth и тип зависимости deprel
        (Категориальный признак: nsubj (подлежащее), amod (определение),
        и др)
    -   частотные, чтобы учесть статистическую значимость терминов.
        Извлекается TF (Term Frequency, Биграммы, Триграммы и частота
        составных навыков
Благодаря глубокой обработке данных (создание качественных эмбеддингов,
грамотному сочетанию возможностей DeepPavlov и spaCy, а также
специализированному объединению составных терминов) достигнута высокая
точность извлечения профессиональных навыков.

## Этап 4: Извлечение навыков через линейные комбинации признаков
Создан класс SkillExtractor, который реализует извлечение
профессиональных навыков из текста, комбинируя семантические,
синтаксические и позиционные признаки. Использует EnhancedSyntaxAnalyzer
для предобработки и синтаксического разбора.
1.  Генерация взвешенных векторов навыков
    Каждый токен преобразуется в композитный вектор, объединяющий три
    ключевых аспекта:
    -   Семантические признаки (вес 0.6): Используются нормализованные
        эмбеддинги из EnhancedSyntaxAnalyzer
    -   Синтаксические признаки (вес 0.3): Используются признаки,
        характеризующие синтаксическую роль слова (is_root корень
        предложения, dependency_depth уровень вложенности в
        синтаксическом дереве, head_position относительная позиция
        головного слова).
    -   Позиционные признаки (вес 0.1): Нормализованный индекс токена
    Итоговый вектор - конкатенация всех признаков с учетом весов
2.  Определение уровня уверенности в принадлежности слова к навыкам.
    Каждой единице приписывается коэффициент уверенности (confidence
    score), зависящий от нормы суммарного вектора и влияния его части
    речи (POS-tag). Высокий вес получают те единицы, которые находятся
    ближе к вершине синтаксического дерева и обладают высокой степенью
    семантической насыщенности.
3.  Расширение списка навыков синонимами. Используются ресурсы WordNet
    для нахождения синонимов, при этом задействуется кэширование для
    повышения производительности.
4.  Постобработка и отбор лучших вариантов Окончательно список навыков
    формируется с учётом:
    -   Порога уверенности: навыки с confidence \< min_confidence
        отбрасываются
    -   Валидации по словарю: токен должен быть в base_skills или
        is_compound=True
    -   Дедупликации: сохраняется вариант с максимальной уверенностью

## Результаты оценки качества модели
Качество проверялось на размеченном golden dataset (220 записей)

Значения метрик:

Precision: 0.789

Recall:  0.729

F1: 0.749

Модель показала сбалансированное качество F1:  0.75,
Precision: 0.789 (из всех выделенных навыков 78.9% действительно являются навыками), 
Recall: 0.729 (модель находит 72.9% всех существующих навыков).

## Заключение
Возможности для улучшения качества модели
1.  Апгрейд языковых моделей: Переход на rubert-tiny2/rubert-base
2.  Доработка словаря, автоматическое пополнение словаря через анализ
    n-грамм (например, биграммы с высокой TF-IDF). Интеграция словаря с
    профессиональными онтологиями
3.  Реализация гибридной архитектуры (NER + Синтаксис). Этап NER:
    Дообученная BertForTokenClassification на размеченных данных. Этап
    постобработки: Синтаксические правила + линейные комбинации

## Список литературы
Николаев И.Е. Метод извлечения знаний и навыков/компетенций из текстов
требований вакансий // Онтология проектирования. 2023. Т.13, №2(48).
С.282-293. DOI:
[10.18287/2223-9537-2023-13-2-282-293](10.18287/2223-9537-2023-13-2-282-293)

KOMAROVA L.A., SOLOVIEV V.I., KOLOSOV A.M. Сравнение языковых моделей в
задаче извлечения навыков из вакансий и резюме // Современные
информационные технологии и ИТ-образование. 2024. Т.20, №1. С.157-163.
ISSN 2411-1473

DeepPavlov Team. Синтаксический парсер DeepPavlov \[Электронный
ресурс\]. URL:
<http://docs.deeppavlov.ai/en/master/features/models/syntaxparser.html>

Dale D. Маленький и быстрый BERT для русского языка \[Электронный
ресурс\] // Habr. 2021. URL: <https://habr.com/ru/post/562064/> 

Burtsev M., Anh Le. A Deep Neural Network Model for the Task of Named
Entity Recognition // International Journal of Machine Learning and
Computing. 2019. Vol.9, No.5. P.620-625

