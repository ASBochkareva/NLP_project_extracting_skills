{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "083b2d63-0c6b-42ae-bc75-b1b8dcc7331e",
   "metadata": {},
   "source": [
    "# NLP project: Extracting skills from job descriptions.\n",
    "# Синтаксический анализ с deeppavlov_syntax_parser + Извлечение навыков через линейные комбинации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ffb588bb-baf4-4ee5-bf32-30ba85f5355b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "import spacy\n",
    "import itertools\n",
    "import re\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from wiki_ru_wordnet import WikiWordnet\n",
    "wwn = WikiWordnet() \n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from deeppavlov import build_model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib as plt\n",
    "\n",
    "import torch\n",
    "import wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bf9722b1-7509-4901-9cfd-71514fc738bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "wwn = WikiWordnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b394064f-8923-4880-8e6b-713d82cc6c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Синонимы: ['управление', 'руководство', 'администрирование']\n"
     ]
    }
   ],
   "source": [
    "# # тест вызова лемма\n",
    "# synonyms = set()\n",
    "\n",
    "# for synset in wwn.get_synsets(\"управление\"):\n",
    "#     for word_obj in synset.get_words():\n",
    "#         synonyms.add(word_obj.lemma())  \n",
    "\n",
    "# print(\"Синонимы:\", list(synonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0c62a3aa-90b8-4ede-97b9-fd5cc9bf070c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedSyntaxAnalyzer:\n",
    "    def __init__(self):\n",
    "        PROFESSIONAL_POS_TAG = 'PROSKILL'  # тег для профессиональных терминов (и составных, и не составных)\n",
    "        self.mlm_model = AutoModel.from_pretrained('./mlm_results/final_model')\n",
    "        self.mlm_tokenizer = AutoTokenizer.from_pretrained('./mlm_results/final_model')\n",
    "        self.syntax_parser = build_model(\"syntax_ru_syntagrus_bert\", download=True)       \n",
    "        self.base_skills, self.compound_skills = self._load_skills()   \n",
    "        self.spacy_nlp = spacy.load(\"ru_core_news_sm\")\n",
    "        self.syntax_cache = defaultdict(list)\n",
    "        self.morph = MorphAnalyzer()\n",
    "        self.stop_words = set(stopwords.words(\"russian\")) # Скачиваем русские стоп-слова (для очистки текста)\n",
    "\n",
    "    def _load_skills(self):\n",
    "        try:\n",
    "            df = pd.read_excel('skills_1706.xlsx')\n",
    "            base_skills = set(df.query('compound_skill != 1')['skills'])\n",
    "            compound_skills = set(df.query('compound_skill == 1')['skills'])\n",
    "            return base_skills, compound_skills\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка при чтении файла компаунд-навыков: {e}\")\n",
    "            return set()\n",
    "    \n",
    "    def get_token_embeddings(self, text):\n",
    "        try:\n",
    "        # Токенизация с получением offset_mapping\n",
    "            inputs = self.mlm_tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                return_offsets_mapping=True,\n",
    "                add_special_tokens=False,\n",
    "                truncation=True\n",
    "            )\n",
    "        \n",
    "        # Отделяем offset_mapping, который не нужен для модели\n",
    "            offset_mapping = inputs.pop('offset_mapping')[0].numpy()\n",
    "        \n",
    "        # Получаем эмбеддинги от модели\n",
    "            with torch.no_grad():\n",
    "                outputs = self.mlm_model(**inputs)       \n",
    "            token_embeddings = outputs.last_hidden_state[0]\n",
    "        \n",
    "        # Группируем токены в слова\n",
    "            word_embeddings = []\n",
    "            current_word = []\n",
    "        \n",
    "            for idx, (start, end) in enumerate(offset_mapping):\n",
    "                if start == end:  # Специальные токены\n",
    "                    continue\n",
    "                \n",
    "            # Получаем текст токена для проверки пунктуации\n",
    "                token_text = self.mlm_tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][idx].item())\n",
    "            \n",
    "            # Обработка пунктуации (добавлено в этом месте)\n",
    "                if token_text in {':', ',', '.', ';'}:\n",
    "                # Добавляем текущее слово, если оно есть\n",
    "                    if current_word:\n",
    "                        avg_emb = np.mean([t['embedding'] for t in current_word], axis=0)\n",
    "                        word_embeddings.append({\n",
    "                            'start': current_word[0]['start'],\n",
    "                            'end': current_word[-1]['end'],\n",
    "                            'embedding': avg_emb\n",
    "                        })\n",
    "                        current_word = []\n",
    "                \n",
    "                # Добавляем пунктуацию как отдельный токен\n",
    "                    word_embeddings.append({\n",
    "                        'start': start,\n",
    "                        'end': end,\n",
    "                        'embedding': token_embeddings[idx].numpy()\n",
    "                    })\n",
    "                    continue\n",
    "                \n",
    "            # Обработка обычных слов\n",
    "                if current_word and start > current_word[-1]['end']:\n",
    "                # Сохраняем собранное слово\n",
    "                    avg_emb = np.mean([t['embedding'] for t in current_word], axis=0)\n",
    "                    word_embeddings.append({\n",
    "                        'start': current_word[0]['start'],\n",
    "                        'end': current_word[-1]['end'],\n",
    "                        'embedding': avg_emb\n",
    "                    })\n",
    "                    current_word = []\n",
    "            \n",
    "                current_word.append({\n",
    "                    'start': start,\n",
    "                    'end': end,\n",
    "                    'embedding': token_embeddings[idx].numpy()\n",
    "                })\n",
    "        \n",
    "        # Добавляем последнее слово\n",
    "            if current_word:\n",
    "                avg_emb = np.mean([t['embedding'] for t in current_word], axis=0)\n",
    "                word_embeddings.append({\n",
    "                    'start': current_word[0]['start'],\n",
    "                    'end': current_word[-1]['end'],\n",
    "                    'embedding': avg_emb\n",
    "                })\n",
    "        \n",
    "            return word_embeddings\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error in get_token_embeddings: {str(e)}\")\n",
    "            return []\n",
    "  \n",
    "    def normalize_term(self, term):\n",
    "        term = term.lower()\n",
    "        replacements = {\n",
    "            'битрикс24': 'bitrix24',\n",
    "            'в2в': 'b2b',\n",
    "            'б2б': 'b2b',\n",
    "            '1с': '1c',\n",
    "            'в2с': 'b2c',\n",
    "            'crm': 'crm',\n",
    "            '1с': '1c',\n",
    "            'в2f': 'b2f',\n",
    "            'б2б': 'b2b'\n",
    "        }\n",
    "        return replacements.get(term, term) \n",
    "\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        # Очистка текста и нормализация    \n",
    "        text = re.sub(r'(?<!\\S)\\d+(?:\\.\\d+)?(?!\\S)', '', text)  # Удаление отдельно стоящих цифр (включая числа с точкой)\n",
    "        text = re.sub(r'([а-яё])([А-ЯЁ])', r'\\1 \\2', text)  # Разделяем CamelCase\n",
    "        text = re.sub(r'([а-яёa-zА-ЯЁA-Z])([()])', r'\\1 \\2', text)  # Отделяем буквы от скобок\n",
    "        text = re.sub(r'([()])([а-яёa-zА-ЯЁA-Z])', r'\\1 \\2', text)  # Отделяем скобки от букв  \n",
    "        text = re.sub(r'<[^>]+>', '', text)  # Удаляем HTML-теги \n",
    "        text = re.sub(r'\\n|\\t|•|\\*', ' ', text)  # Удаляем переводы строк, табуляторы и маркеры списка\n",
    "        text = re.sub('ё', 'е', text)  # Замена буквы ё на е\n",
    "        text = re.sub(r'([.,;:])\\s*-', r'\\1', text)  # Убираем тире после знаков препинания\n",
    "        text = re.sub(r'\\s-', '-', text)  # Убираем пробел перед тире\n",
    "        text = re.sub(r'-\\s', '-', text)  # Убираем пробел после тире\n",
    "        text = re.sub(r'([а-яёa-zА-ЯЁA-Z])(-\\s*)([а-яёa-zА-ЯЁA-Z])', r'\\1-\\3', text)  # Склеиваем слова с дефисом\n",
    "        text = re.sub(r'[^\\w\\s.%&,/\\-{}$$:;\\']+', '', text)  # Оставляем нужные символы\n",
    "        text = re.sub(r'\\s+', ' ', text) #удаление возможных двойных пробелов после удаления цифр\n",
    "        text = text.lower().strip()\n",
    "\n",
    "        #Удаление стоп-слов\n",
    "        words = text.split()\n",
    "        filtered_words = [w for w in words if w not in self.stop_words]\n",
    "        text = ' '.join(filtered_words)\n",
    "\n",
    "        # Замена профессиональных терминов\n",
    "        for term in sorted(self.base_skills, key=len, reverse=True):\n",
    "            text = re.sub(rf'\\b{re.escape(term)}\\b', self.normalize_term(term), text, flags=re.IGNORECASE)\n",
    "\n",
    "        return text\n",
    "        \n",
    "    def merge_compounds(self, parsed_data):\n",
    "   \n",
    "        if not parsed_data:\n",
    "            return []\n",
    "\n",
    "        i = 0\n",
    "        n = len(parsed_data)\n",
    "\n",
    "        while i < n - 1:\n",
    "            current = parsed_data[i]\n",
    "            next_item = parsed_data[i+1] if i+1 < n else None\n",
    "\n",
    "            # Проверяем составные навыки\n",
    "            for length in range(min(3, n-i), 1, -1):\n",
    "                phrase = ' '.join([parsed_data[j]['word'].lower() for j in range(i, i+length)])\n",
    "\n",
    "                # Проверяем, что фраза находится в compound_skills И НИ ОДИН из токенов не является базовым навыком\n",
    "                if (phrase in self.compound_skills and \n",
    "                    not any(parsed_data[j]['word'].lower() in self.base_skills for j in range(i, i+length))):\n",
    "\n",
    "                    # Объединяем токены, если образуется составной навык\n",
    "                    start_pos = parsed_data[i]['start']\n",
    "                    end_pos = parsed_data[i+length-1]['end']\n",
    "\n",
    "                    parsed_data[i] = {\n",
    "                        **current,\n",
    "                        'word': '_'.join([parsed_data[j]['word'] for j in range(i, i+length)]),\n",
    "                        'embedding': np.mean([parsed_data[j]['embedding'] for j in range(i, i+length)], axis=0),\n",
    "                        'is_compound': True,\n",
    "                        'start': start_pos,\n",
    "                        'end': end_pos,\n",
    "                        'pos': 'PROSKILL'\n",
    "                    }\n",
    "\n",
    "                # Удаляем объединенные токены\n",
    "                    for _ in range(length-1):\n",
    "                        del parsed_data[i+1]\n",
    "                    n -= (length - 1)\n",
    "                    break\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        return parsed_data\n",
    "\n",
    "    def parse_sentence_dependencies(self, sentence):\n",
    "        doc = self.spacy_nlp(sentence)\n",
    "        return [(w.text, w.dep_, w.head.text) for w in doc]\n",
    "    def analyze_large_contexts(self, sentences):\n",
    "    # Генерация биграмм (пар соседних предложений)\n",
    "        bigrams = list(zip(sentences[:-1], sentences[1:]))\n",
    "    # Генерация триграмм (тройки соседних предложений)\n",
    "        trigrams = list(zip(sentences[:-2], sentences[1:-1], sentences[2:]))\n",
    "        return bigrams, trigrams\n",
    "\n",
    "\n",
    "    def calculate_frequency_features(self, tokens):\n",
    "        document = ' '.join(tokens)    \n",
    "        self.vectorizer = CountVectorizer(\n",
    "            ngram_range=(1, 1),\n",
    "            token_pattern=r'(?u)\\b\\w+\\b'\n",
    "        )   \n",
    "        freq_matrix = self.vectorizer.fit_transform([document])\n",
    "        return dict(zip(\n",
    "            self.vectorizer.get_feature_names_out(),\n",
    "            freq_matrix.toarray()[0]\n",
    "    )) \n",
    "        \n",
    "    def extract_significant_patterns(self, data):\n",
    "        significant_patterns = []\n",
    "        for entry in data:\n",
    "            sentence = entry[\"sentence\"]\n",
    "            doc = self.spacy_nlp(sentence)\n",
    "            for token in doc:\n",
    "            # Расширяем список зависимостей\n",
    "                if token.dep_ in {\"obj\", \"obl\", \"nsubj\", \"attr\", \"advcl\", \"xcomp\", \"acl\", \"amod\", \"compound\", \"conj\",  \"pobj\"}:                                \n",
    "                    pattern = token.text\n",
    "                    significant_patterns.append(pattern)\n",
    "        return significant_patterns \n",
    "        \n",
    "    def enhanced_parse_with_features(self, text):\n",
    "        try:\n",
    "        #Предварительная обработка\n",
    "            processed_text = self.preprocess_text(text)\n",
    "        \n",
    "        #Получение эмбеддингов\n",
    "            word_embeddings = self.get_token_embeddings(processed_text)\n",
    "        \n",
    "        #Анализ через spaCy\n",
    "            doc = self.spacy_nlp(processed_text)\n",
    "            spacy_deps = [(token.text, token.dep_, token.head.text) for token in doc]\n",
    "        \n",
    "        #Анализ через DeepPavlov\n",
    "            syntax_result = self.syntax_parser([processed_text])[0]\n",
    "            lines = [line for line in syntax_result.strip().split('\\n') \n",
    "                    if line and not line.startswith('#')]\n",
    "        \n",
    "        #Сопоставление данных\n",
    "            parsed_data = []\n",
    "            text_pos = 0\n",
    "        \n",
    "            for line in lines:\n",
    "                parts = line.split('\\t')\n",
    "                if len(parts) < 8:\n",
    "                    continue\n",
    "                \n",
    "                word = parts[1]\n",
    "                word_start = processed_text.find(word, text_pos)\n",
    "                if word_start == -1:\n",
    "                    continue\n",
    "                \n",
    "                word_end = word_start + len(word)\n",
    "                text_pos = word_end\n",
    "            \n",
    "            # Поиск соответствующего эмбеддинга\n",
    "                embedding = next(\n",
    "                    (emb['embedding'] for emb in word_embeddings\n",
    "                    if emb['start'] <= word_start and emb['end'] >= word_end\n",
    "                ), None)\n",
    "            \n",
    "                parsed_data.append({\n",
    "                    'id': int(parts[0]),\n",
    "                    'word': word,\n",
    "                    'lemma': parts[2],\n",
    "                    'pos': parts[3],\n",
    "                    'features': parts[5],\n",
    "                    'head': int(parts[6]),\n",
    "                    'deprel': parts[7],\n",
    "                    'embedding': embedding,\n",
    "                    'is_compound': False,\n",
    "                    'spacy_dep': next((d for d in spacy_deps if d[0] == word), None),\n",
    "                    'start': word_start,\n",
    "                    'end': word_end\n",
    "                })\n",
    "        \n",
    "        # Корректировка пунктуации\n",
    "            for i, token in enumerate(parsed_data):\n",
    "                if token['word'] in {':', ',', '.', ';'} and i > 0:\n",
    "                    token['head'] = parsed_data[i-1]['id']\n",
    "                    token['deprel'] = 'punct'\n",
    "\n",
    "\n",
    "            # Заполнение POS-тегов через spaCy\n",
    "            doc = self.spacy_nlp(processed_text)\n",
    "            for token in parsed_data:\n",
    "                # Ищем соответствующий токен в spaCy по позиции start\n",
    "                spacy_token = next(\n",
    "                    (t for t in doc if t.idx == token['start']), \n",
    "                    None\n",
    "                )\n",
    "                if spacy_token:\n",
    "                    token['pos'] = spacy_token.pos_  # Заполняем только POS-тег\n",
    "\n",
    "            # Упрощённая лемматизация через единый метод\n",
    "            for token in parsed_data:\n",
    "                token['lemma'] = self.get_lemma(token['word'], token.get('pos'))\n",
    "            \n",
    "                # Для отладки (можно удалить)\n",
    "                if token['word'].lower() == 'управление':\n",
    "                    print(f\"Лемма для 'управление': {token['lemma']}\")\n",
    "                \n",
    "        #Объединение составных терминов\n",
    "            parsed_data = self.merge_compounds(parsed_data)\n",
    "\n",
    "            #Разметка профессиональных терминов\n",
    "            for token in parsed_data:\n",
    "            # Проверяем как оригинальные термины, так и нормализованные варианты\n",
    "                normalized_word = self.normalize_term(token['word'].lower())\n",
    "                if (token['word'].lower() in self.base_skills or \n",
    "                    normalized_word in self.base_skills):\n",
    "                    token['pos'] = 'PROSKILL'\n",
    "                    \n",
    "       # Добавление признаков\n",
    "            for token in parsed_data:\n",
    "                token['syntax_features'] = {\n",
    "                    'head_position': token['head'] - token['id'],\n",
    "                    'is_root': token['head'] == 0,\n",
    "                    'dependency_depth': self._calculate_depth(parsed_data, token['id'])\n",
    "                }\n",
    "            \n",
    "                if token['embedding'] is not None:\n",
    "                    norm = np.linalg.norm(token['embedding'])\n",
    "                    token['normalized_embedding'] = token['embedding'] / norm if norm > 0 else token['embedding']\n",
    "        \n",
    "        # Формирование результата\n",
    "            return {\n",
    "                'tokens': parsed_data,\n",
    "                'spacy_doc': doc,\n",
    "                'embeddings': word_embeddings,\n",
    "                'dependencies': spacy_deps,\n",
    "                'freq_features': self.calculate_frequency_features([t.text for t in doc]),\n",
    "                'bigrams': list(zip(doc[:-1], doc[1:])),\n",
    "                'trigrams': list(zip(doc[:-2], doc[1:-1], doc[2:])),\n",
    "                'compound_skills': [t for t in parsed_data if t.get('is_compound', False)]\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error in enhanced_parse_with_features: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def _calculate_depth(self, parsed_data, token_id):\n",
    "        depth = 0\n",
    "        while True:\n",
    "            token = next((t for t in parsed_data if t['id'] == token_id), None)\n",
    "            if token is None or token['head'] == 0:\n",
    "                break\n",
    "            depth += 1\n",
    "            token_id = token['head']\n",
    "        return depth\n",
    "        \n",
    "    def get_lemma(self, word: str, pos: str = None) -> str:\n",
    "        word = str(word).strip()\n",
    "        if not word:\n",
    "            return word\n",
    "\n",
    "        # Пропускаем специальные токены\n",
    "        if pos in ['PUNCT', 'NUM', 'X']:\n",
    "            return word.lower()\n",
    "\n",
    "        # Обработка составных слов\n",
    "        if '_' in word:\n",
    "            parts = word.split('_')\n",
    "            return '_'.join([self.get_lemma(part, pos) for part in parts])\n",
    "\n",
    "        # Нормализация профессиональных терминов\n",
    "        normalized = self.normalize_term(word.lower())\n",
    "        if normalized != word.lower():\n",
    "            return normalized\n",
    "\n",
    "        # Для существительных пробуем WordNet\n",
    "        if pos in [None, 'NOUN', 'PROSKILL']:\n",
    "            try:\n",
    "                synsets = self.wiki_wordnet.get_synsets(normalized)\n",
    "                if synsets:\n",
    "                    first_lemma = synsets[0].get_words()[0].lemma()\n",
    "                    if first_lemma:\n",
    "                        return first_lemma.lower()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        # Основная лемматизация через pymorphy2\n",
    "        try:\n",
    "            parsed = self.morph.parse(word)\n",
    "            if not parsed:\n",
    "                return word.lower()\n",
    "            \n",
    "            # Учитываем часть речи при лемматизации\n",
    "            return parsed[0].normal_form if pos is None else \\\n",
    "                   next((p.normal_form for p in parsed if p.tag.POS == pos), \n",
    "                        parsed[0].normal_form)\n",
    "        except:\n",
    "            return word.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930993ab-b061-4c13-ab17-9297f200e4a5",
   "metadata": {},
   "source": [
    "# Извлечение навыков через линейные комбинации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1b471f01-4a88-47ce-8dca-abdc8e7df772",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.corpus import wordnet\n",
    "from itertools import chain\n",
    "import re\n",
    "from string import punctuation, digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "36869b2f-43ad-46dd-b3ab-371094ccb834",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkillExtractor:\n",
    "    def __init__(self, analyzer=None, weights=None, enable_synonyms=True):\n",
    "\n",
    "        self.analyzer = analyzer or EnhancedSyntaxAnalyzer()\n",
    "        self.enable_synonyms = enable_synonyms  \n",
    "        if self.enable_synonyms:\n",
    "            self.wiki_wordnet = WikiWordnet()\n",
    "            self._synonyms_cache = {}\n",
    "            \n",
    "        self.wiki_wordnet = WikiWordnet()\n",
    "        self._synonyms_cache = {}\n",
    "        \n",
    "        # Настройка весов признаков\n",
    "        # Валидация весов\n",
    "        if weights:\n",
    "            if not all(0 <= v <= 1 for v in weights.values()):\n",
    "                raise ValueError(\"Веса должны быть в диапазоне [0, 1]\")\n",
    "            if not np.isclose(sum(weights.values()), 1.0):\n",
    "                raise ValueError(\"Сумма весов должна быть равна 1\")\n",
    "        self.weights = weights or {'semantic': 0.6, 'syntax': 0.3, 'position': 0.1}\n",
    "        \n",
    "        # Кеширование базовых навыков\n",
    "        self._base_skills = None\n",
    "      #  self.pca = PCA(n_components=50) пока не использую\n",
    "        \n",
    "    @property\n",
    "    def base_skills(self):\n",
    "        \"\"\"Ленивая загрузка base_skills\"\"\"\n",
    "        if self._base_skills is None:\n",
    "            self._base_skills = self.analyzer.base_skills\n",
    "        return self._base_skills\n",
    "\n",
    "    # Метод создания комбинированного представления токена\n",
    "    def create_combined_representation(self, token, total_tokens):\n",
    "    # Проверяем наличие normalized_embedding\n",
    "        if \"normalized_embedding\" not in token:\n",
    "            raise ValueError(\"Токен не содержит normalized_embedding\")\n",
    "\n",
    "    # Получаем синтаксические признаки (корректно из syntax_features)\n",
    "        syntax_features = [\n",
    "            float(token[\"syntax_features\"][\"is_root\"]),\n",
    "            float(token[\"syntax_features\"][\"dependency_depth\"]),\n",
    "            float(token[\"syntax_features\"][\"head_position\"])\n",
    "        ]\n",
    "\n",
    "    # Позиционная характеристика (индекс берем из id токена)\n",
    "        position_feature = [token[\"id\"] / total_tokens]\n",
    "\n",
    "    # Комбинируем признаки с весами\n",
    "        combined_vector = np.concatenate((\n",
    "            token[\"normalized_embedding\"] * self.weights['semantic'],\n",
    "            np.array(syntax_features) * self.weights['syntax'],\n",
    "            np.array(position_feature) * self.weights['position']\n",
    "        ))\n",
    "\n",
    "    # Нормализация\n",
    "        norm = np.linalg.norm(combined_vector)\n",
    "        return combined_vector / norm if norm > 0 else combined_vector\n",
    "\n",
    "    # Основной метод извлечения навыков\n",
    "    def extract_skills(self, text, min_confidence=None):\n",
    "        if not text or not isinstance(text, str):\n",
    "            return {'skills': [], 'confidence_scores': []}\n",
    "    \n",
    "        try:\n",
    "            analysis_result = self.analyzer.enhanced_parse_with_features(text)\n",
    "            if not analysis_result:\n",
    "                return {'skills': [], 'confidence_scores': []}\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка анализа текста: {str(e)}\")\n",
    "            return {'skills': [], 'confidence_scores': []}\n",
    "\n",
    "        # Обработка токенов\n",
    "        skills_data = self._process_tokens(analysis_result['tokens'])\n",
    "    \n",
    "        if not skills_data['skills']:\n",
    "            return {'skills': [], 'confidence_scores': []}\n",
    "\n",
    "        # Нормализация confidence scores [0, 1]\n",
    "        confidences = skills_data['confidences']\n",
    "        if np.max(confidences) > 0:\n",
    "            confidences = confidences / np.max(confidences)\n",
    "    \n",
    "        # Автоматический расчет порога\n",
    "        if min_confidence is None:\n",
    "            min_confidence = self.calculate_auto_threshold(confidences)\n",
    "    \n",
    "        # Фильтрация\n",
    "        valid_indices = np.where(confidences >= min_confidence)[0]\n",
    "        filtered_skills = [skills_data['skills'][i] for i in valid_indices]\n",
    "        filtered_confidences = confidences[valid_indices]\n",
    "    \n",
    "        return {\n",
    "            'skills': self.postprocess_skills({\n",
    "                'raw_skills': filtered_skills,\n",
    "                'confidences': filtered_confidences,\n",
    "                'original_text': text\n",
    "            }),\n",
    "            'confidence_scores': filtered_confidences\n",
    "        }\n",
    "\n",
    "    def _process_tokens(self, tokens):\n",
    "        skills = []\n",
    "        representations = []\n",
    "        confidences = []\n",
    "    \n",
    "        for token in tokens:\n",
    "            if not self._is_relevant_token(token):\n",
    "                continue\n",
    "            \n",
    "            rep = self.create_combined_representation(token, len(tokens))\n",
    "            confidence = self._calculate_confidence(rep, token)\n",
    "        \n",
    "            skills.append({\n",
    "                'text': token['word'],\n",
    "                'is_compound': token.get('is_compound', False),\n",
    "                'pos': token['pos'],\n",
    "                'embedding': rep\n",
    "            })\n",
    "            representations.append(rep)\n",
    "            confidences.append(confidence)\n",
    "    \n",
    "        return {\n",
    "            'skills': skills,\n",
    "            'confidences': np.array(confidences),\n",
    "            'representations': representations\n",
    "        }\n",
    "\n",
    "    def _is_relevant_token(self, token):\n",
    "       # \"\"\"Проверка релевантности токена\"\"\"\n",
    "        return token['pos'] in ['PROSKILL', 'NOUN'] and 'normalized_embedding' in token\n",
    "\n",
    "    def _calculate_confidence(self, representation, token):\n",
    "      #  \"\"\"Вычисление уверенности с кешированием\"\"\"\n",
    "        pos_weight = 1.5 if token['pos'] == 'PROSKILL' else 1.0\n",
    "        return np.linalg.norm(representation) * pos_weight\n",
    "\n",
    "    def postprocess_skills(self, data):\n",
    "        if not data['raw_skills']:\n",
    "            return []\n",
    "\n",
    "        # Дедупликация\n",
    "        unique_skills = {}\n",
    "        for skill, conf in zip(data['raw_skills'], data['confidences']):\n",
    "            skill_text = skill['text'].lower()\n",
    "            if skill_text not in unique_skills or conf > unique_skills[skill_text]['confidence']:\n",
    "                skill['confidence'] = conf\n",
    "                unique_skills[skill_text] = skill\n",
    "\n",
    "        # Валидация\n",
    "        base_skills_set = {s.lower() for s in self.base_skills}\n",
    "        validated = []\n",
    "        \n",
    "        for skill in unique_skills.values():\n",
    "            if (skill['text'].lower() in base_skills_set or \n",
    "                skill.get('is_compound', False)):\n",
    "                \n",
    "                validated.append(skill)\n",
    "                \n",
    "                # Добавляем синонимы только если флаг включен\n",
    "                if self.enable_synonyms and skill['text'].lower() in base_skills_set:\n",
    "                    for synonym in self._get_cached_synonyms(skill['text']):\n",
    "                        if synonym != skill['text'].lower():\n",
    "                            new_skill = skill.copy()\n",
    "                            new_skill['text'] = synonym\n",
    "                            new_skill['is_synonym'] = True\n",
    "                            validated.append(new_skill)\n",
    "\n",
    "        return sorted(validated, key=lambda x: -x['confidence'])   \n",
    "         \n",
    "    def calculate_auto_threshold(self, confidences):\n",
    "    # \"\"\"Автоматический расчет порога уверенности по методу межквартильного размаха\"\"\"\n",
    "        if not len(confidences):\n",
    "            return 0.0\n",
    "    \n",
    "        confidences = np.array(confidences)\n",
    "        q1 = np.percentile(confidences, 25)\n",
    "        q3 = np.percentile(confidences, 75)\n",
    "        iqr = q3 - q1\n",
    "        return max(0.1, q1 - 1.5 * iqr)  # Нижняя граница 0.1\n",
    "\n",
    "    def get_synonyms(self, word: str) -> list:\n",
    "        normalized = self.normalize_term(word.lower())\n",
    "        synonyms = set()\n",
    "    \n",
    "        try:\n",
    "            # Способ 1: Через официальное API\n",
    "            for synset in self.wiki_wordnet.get_synsets(normalized):\n",
    "                for word_obj in synset.get_words():\n",
    "                    try:\n",
    "                        # Правильное получение леммы через вызов метода\n",
    "                        lemma = word_obj.lemma().lower()\n",
    "                        synonyms.add(lemma)\n",
    "                    except AttributeError:\n",
    "                        # Если метод lemma() не доступен, пытаемся получить данные через _data\n",
    "                        if hasattr(word_obj, '_data'):\n",
    "                            lemma = word_obj._data.get('lemma', '').lower()\n",
    "                            if lemma:\n",
    "                                synonyms.add(lemma)\n",
    "        \n",
    "            # Способ 2: Через прямое обращение к данным synset (дополнительная проверка)\n",
    "            for synset in self.wiki_wordnet.get_synsets(normalized):\n",
    "                if hasattr(synset, '_data'):\n",
    "                    for word_data in synset._data.get('words', []):\n",
    "                        if isinstance(word_data, dict):\n",
    "                            lemma = word_data.get('lemma', '').lower()\n",
    "                            if lemma:\n",
    "                                synonyms.add(lemma)\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка при поиске синонимов для '{word}': {str(e)}\")\n",
    "    \n",
    "        # Всегда включаем нормализованную форму исходного слова\n",
    "        synonyms.add(normalized)\n",
    "    \n",
    "        # Фильтрация и сортировка результатов\n",
    "        filtered = [s for s in synonyms if s and len(s) > 1]  # Игнорируем пустые и слишком короткие\n",
    "        return sorted(filtered, key=lambda x: (x != normalized, x))  # Исходное слово первое\n",
    "\n",
    "    def _expand_compound_synonyms(self, word: str) -> list:\n",
    "        # \"\"\"Отдельный метод для обработки составных терминов\"\"\"\n",
    "        if '_' not in word:\n",
    "            return self.get_synonyms(word)\n",
    "            \n",
    "        components = word.split('_')\n",
    "        synonyms = set()\n",
    "        for comp in components:\n",
    "            synonyms.update(self.get_synonyms(comp))\n",
    "        return sorted(synonyms)\n",
    "\n",
    "    def expand_with_synonyms(self, skill_data: dict) -> list:\n",
    "        # Основной метод расширения навыков синонимами.\n",
    "        expanded = []\n",
    "        \n",
    "        # Базовые синонимы\n",
    "        base_synonyms = self._get_cached_synonyms(skill_data['text'])\n",
    "        \n",
    "        for synonym in base_synonyms:\n",
    "            new_skill = skill_data.copy()\n",
    "            new_skill['text'] = synonym\n",
    "            new_skill['is_synonym'] = (synonym != skill_data['text'])\n",
    "            expanded.append(new_skill)\n",
    "        \n",
    "        return expanded\n",
    "\n",
    "    def _get_cached_synonyms(self, term: str) -> list:\n",
    "        #Кеширующий метод для получения синонимов\n",
    "        normalized = self.analyzer.normalize_term(term.lower())\n",
    "        \n",
    "        if normalized not in self._synonyms_cache:\n",
    "            self._synonyms_cache[normalized] = self._fetch_raw_synonyms(normalized)\n",
    "            \n",
    "        return self._synonyms_cache[normalized]\n",
    "\n",
    "    def _fetch_raw_synonyms(self, term: str) -> list:\n",
    "        #Низкоуровневое получение синонимов из WordNet\n",
    "        synonyms = set()\n",
    "        \n",
    "        try:\n",
    "            for synset in self.wiki_wordnet.get_synsets(term):\n",
    "                for word_obj in synset.get_words():\n",
    "                    lemma = word_obj.lemma().lower() if callable(word_obj.lemma) else str(word_obj.lemma).lower()\n",
    "                    if lemma:\n",
    "                        synonyms.add(lemma)\n",
    "        except Exception as e:\n",
    "            print(f\"Synonym lookup error for {term}: {str(e)}\")\n",
    "        \n",
    "        synonyms.add(term)  # Всегда включаем исходный термин\n",
    "        return sorted(s for s in synonyms if s and len(s) > 1)\n",
    "\n",
    "    def plot_skills_confidence(self, skills_data):\n",
    "       # \"\"\"Визуализация уверенности в навыках\"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "    \n",
    "        skills = [s['text'] for s in skills_data['skills']]\n",
    "        confidences = skills_data['confidence_scores']\n",
    "    \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.barh(skills, confidences)\n",
    "        plt.xlabel('Уверенность')\n",
    "        plt.title('Распределение уверенности в навыках')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "9c12e557-5cdc-462a-90ba-fe98a0e4e140",
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_dataset = pd.read_excel('golden_dataset_1706.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "2843d99b-fff8-45f5-9acf-bbe4c691b0b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(427, 5)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "golden_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "669bda0f-c367-499b-8ec2-34049615fd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "cb05ca71-faec-4b15-ac82-3a15ac3f8151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_requirements</th>\n",
       "      <th>description_skills</th>\n",
       "      <th>description_bio</th>\n",
       "      <th>num_skills</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>требования: грамотная устная речь ответственно...</td>\n",
       "      <td>грамотная устная речь; ответственность</td>\n",
       "      <td>требования O : O грамотная B-SKILL устная I-SK...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>требования: желательно знание 1с условия: граф...</td>\n",
       "      <td>знание 1с</td>\n",
       "      <td>требования O : O желательно O знание B-SKILL 1...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>требования: образование не ниже среднего. акти...</td>\n",
       "      <td>активный пользователь пк</td>\n",
       "      <td>требования O : O образование O не O ниже O сре...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>требования: -внимательность, ответственность; ...</td>\n",
       "      <td>внимательность; ответственность; уверенный пол...</td>\n",
       "      <td>требования O : O - O внимательность B-SKILL , ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>требования: грамотная речь.</td>\n",
       "      <td>грамотная речь</td>\n",
       "      <td>требования O : O грамотная B-SKILL речь I-SKIL...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    job_requirements  \\\n",
       "0  требования: грамотная устная речь ответственно...   \n",
       "1  требования: желательно знание 1с условия: граф...   \n",
       "2  требования: образование не ниже среднего. акти...   \n",
       "3  требования: -внимательность, ответственность; ...   \n",
       "4                        требования: грамотная речь.   \n",
       "\n",
       "                                  description_skills  \\\n",
       "0             грамотная устная речь; ответственность   \n",
       "1                                          знание 1с   \n",
       "2                           активный пользователь пк   \n",
       "3  внимательность; ответственность; уверенный пол...   \n",
       "4                                     грамотная речь   \n",
       "\n",
       "                                     description_bio  num_skills  \n",
       "0  требования O : O грамотная B-SKILL устная I-SK...           2  \n",
       "1  требования O : O желательно O знание B-SKILL 1...           1  \n",
       "2  требования O : O образование O не O ниже O сре...           1  \n",
       "3  требования O : O - O внимательность B-SKILL , ...           3  \n",
       "4  требования O : O грамотная B-SKILL речь I-SKIL...           1  "
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "golden_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "98186edc-007e-4f4a-83ec-d1d0fa6acfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Нормализация навыков в description_skills (ground truth)\n",
    "def normalize_skills(skills_str):\n",
    "    skills = []\n",
    "    for skill in skills_str.split(';'):\n",
    "        skill = skill.strip()\n",
    "        if not skill:\n",
    "            continue\n",
    "        # Заменяем пробелы на _ в многословных навыках\n",
    "        if len(skill.split()) > 1:\n",
    "            skill = skill.replace(' ', '_')\n",
    "        skills.append(skill.lower())  # Приводим к нижнему регистру\n",
    "    return skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "89ab79a5-4803-4eb3-b14a-120f137f739d",
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_dataset['true_skills'] = golden_dataset['description_skills'].apply(normalize_skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "d0e472a9-7125-4e0d-abf1-a6989aea1278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>job_requirements</th>\n",
       "      <th>description_skills</th>\n",
       "      <th>description_bio</th>\n",
       "      <th>num_skills</th>\n",
       "      <th>true_skills</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>требования: грамотная устная речь ответственно...</td>\n",
       "      <td>грамотная устная речь; ответственность</td>\n",
       "      <td>требования O : O грамотная B-SKILL устная I-SK...</td>\n",
       "      <td>2</td>\n",
       "      <td>[грамотная_устная_речь, ответственность]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>требования опыт работы знание 1с8.2; уверенный...</td>\n",
       "      <td>опыт работы; знание 1с8.2; уверенный пользоват...</td>\n",
       "      <td>требования O опыт B-SKILL работы I-SKILL знани...</td>\n",
       "      <td>6</td>\n",
       "      <td>[опыт_работы, знание_1с8.2, уверенный_пользова...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>требования:образование не ниже среднего профес...</td>\n",
       "      <td>управленческий опыт; опыт в ритейле; пользоват...</td>\n",
       "      <td>требования O : O образование O не O ниже O сре...</td>\n",
       "      <td>3</td>\n",
       "      <td>[управленческий_опыт, опыт_в_ритейле, пользова...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>требования:ответственность, дисциплинированнос...</td>\n",
       "      <td>ответственность; дисциплинированность; внимате...</td>\n",
       "      <td>требования O : O ответственность B-SKILL , O д...</td>\n",
       "      <td>4</td>\n",
       "      <td>[ответственность, дисциплинированность, внимат...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>требования: умение вести переговоры нацеленнос...</td>\n",
       "      <td>умение вести переговоры; нацеленность на резул...</td>\n",
       "      <td>требования O : O умение B-SKILL вести I-SKILL ...</td>\n",
       "      <td>3</td>\n",
       "      <td>[умение_вести_переговоры, нацеленность_на_резу...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                   job_requirements  \\\n",
       "0           2  требования: грамотная устная речь ответственно...   \n",
       "1           6  требования опыт работы знание 1с8.2; уверенный...   \n",
       "2          12  требования:образование не ниже среднего профес...   \n",
       "3          25  требования:ответственность, дисциплинированнос...   \n",
       "4          28  требования: умение вести переговоры нацеленнос...   \n",
       "\n",
       "                                  description_skills  \\\n",
       "0             грамотная устная речь; ответственность   \n",
       "1  опыт работы; знание 1с8.2; уверенный пользоват...   \n",
       "2  управленческий опыт; опыт в ритейле; пользоват...   \n",
       "3  ответственность; дисциплинированность; внимате...   \n",
       "4  умение вести переговоры; нацеленность на резул...   \n",
       "\n",
       "                                     description_bio  num_skills  \\\n",
       "0  требования O : O грамотная B-SKILL устная I-SK...           2   \n",
       "1  требования O опыт B-SKILL работы I-SKILL знани...           6   \n",
       "2  требования O : O образование O не O ниже O сре...           3   \n",
       "3  требования O : O ответственность B-SKILL , O д...           4   \n",
       "4  требования O : O умение B-SKILL вести I-SKILL ...           3   \n",
       "\n",
       "                                         true_skills  \n",
       "0           [грамотная_устная_речь, ответственность]  \n",
       "1  [опыт_работы, знание_1с8.2, уверенный_пользова...  \n",
       "2  [управленческий_опыт, опыт_в_ритейле, пользова...  \n",
       "3  [ответственность, дисциплинированность, внимат...  \n",
       "4  [умение_вести_переговоры, нацеленность_на_резу...  "
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "golden_dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "0717a96d-b825-4036-915c-ac5c143c25e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>job_requirements</th>\n",
       "      <th>description_skills</th>\n",
       "      <th>description_bio</th>\n",
       "      <th>num_skills</th>\n",
       "      <th>true_skills</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>требования: грамотная устная речь ответственно...</td>\n",
       "      <td>грамотная устная речь; ответственность</td>\n",
       "      <td>требования O : O грамотная B-SKILL устная I-SK...</td>\n",
       "      <td>2</td>\n",
       "      <td>[грамотная_устная_речь, ответственность]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>требования опыт работы знание 1с8.2; уверенный...</td>\n",
       "      <td>опыт работы; знание 1с8.2; уверенный пользоват...</td>\n",
       "      <td>требования O опыт B-SKILL работы I-SKILL знани...</td>\n",
       "      <td>6</td>\n",
       "      <td>[опыт_работы, знание_1с8.2, уверенный_пользова...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>требования:образование не ниже среднего профес...</td>\n",
       "      <td>управленческий опыт; опыт в ритейле; пользоват...</td>\n",
       "      <td>требования O : O образование O не O ниже O сре...</td>\n",
       "      <td>3</td>\n",
       "      <td>[управленческий_опыт, опыт_в_ритейле, пользова...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>требования:ответственность, дисциплинированнос...</td>\n",
       "      <td>ответственность; дисциплинированность; внимате...</td>\n",
       "      <td>требования O : O ответственность B-SKILL , O д...</td>\n",
       "      <td>4</td>\n",
       "      <td>[ответственность, дисциплинированность, внимат...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>требования: умение вести переговоры нацеленнос...</td>\n",
       "      <td>умение вести переговоры; нацеленность на резул...</td>\n",
       "      <td>требования O : O умение B-SKILL вести I-SKILL ...</td>\n",
       "      <td>3</td>\n",
       "      <td>[умение_вести_переговоры, нацеленность_на_резу...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                   job_requirements  \\\n",
       "0           2  требования: грамотная устная речь ответственно...   \n",
       "1           6  требования опыт работы знание 1с8.2; уверенный...   \n",
       "2          12  требования:образование не ниже среднего профес...   \n",
       "3          25  требования:ответственность, дисциплинированнос...   \n",
       "4          28  требования: умение вести переговоры нацеленнос...   \n",
       "\n",
       "                                  description_skills  \\\n",
       "0             грамотная устная речь; ответственность   \n",
       "1  опыт работы; знание 1с8.2; уверенный пользоват...   \n",
       "2  управленческий опыт; опыт в ритейле; пользоват...   \n",
       "3  ответственность; дисциплинированность; внимате...   \n",
       "4  умение вести переговоры; нацеленность на резул...   \n",
       "\n",
       "                                     description_bio  num_skills  \\\n",
       "0  требования O : O грамотная B-SKILL устная I-SK...           2   \n",
       "1  требования O опыт B-SKILL работы I-SKILL знани...           6   \n",
       "2  требования O : O образование O не O ниже O сре...           3   \n",
       "3  требования O : O ответственность B-SKILL , O д...           4   \n",
       "4  требования O : O умение B-SKILL вести I-SKILL ...           3   \n",
       "\n",
       "                                         true_skills  \n",
       "0           [грамотная_устная_речь, ответственность]  \n",
       "1  [опыт_работы, знание_1с8.2, уверенный_пользова...  \n",
       "2  [управленческий_опыт, опыт_в_ритейле, пользова...  \n",
       "3  [ответственность, дисциплинированность, внимат...  \n",
       "4  [умение_вести_переговоры, нацеленность_на_резу...  "
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "golden_dataset_test = golden_dataset.iloc[:221,:]\n",
    "golden_dataset_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "dd29268a-444f-44aa-9308-e49dde382884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 6)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "golden_dataset_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "ad3b8b0e-18fd-49f7-a7d2-83dc845c87b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./mlm_results/final_model were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at ./mlm_results/final_model and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-06-17 22:42:10.128 INFO in 'deeppavlov.download'['download'] at line 138: Skipped http://files.deeppavlov.ai/deeppavlov_data/syntax_parsing/rus_6layers.tar.gz download because of matching hashes\n",
      "2025-06-17 22:42:10.707 INFO in 'deeppavlov.download'['download'] at line 138: Skipped http://files.deeppavlov.ai/deeppavlov_data/morpho_tagger/UD2.3/ru_syntagrus.tar.gz download because of matching hashes\n",
      "C:\\PyEnvs\\clean_env\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2025-06-17 22:42:13.699 WARNING in 'deeppavlov.core.models.torch_model'['torch_model'] at line 96: Unable to place component TorchTransformersSyntaxParser on GPU, since no CUDA GPUs are available. Using CPU.\n"
     ]
    }
   ],
   "source": [
    "analyzer = EnhancedSyntaxAnalyzer()\n",
    "extractor = SkillExtractor(analyzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0b113868-73ea-4af5-9c04-ac0319d07e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_skills(text):\n",
    "    try:\n",
    "        result = extractor.extract_skills(text)\n",
    "        \n",
    "        # Нормализуем предсказания так же, как ground truth\n",
    "        pred_skills = []\n",
    "        for skill in result['skills']:\n",
    "            skill_text = skill['text'].lower().strip()\n",
    "            \n",
    "            # Пропускаем пустые навыки\n",
    "            if not skill_text:\n",
    "                continue\n",
    "                \n",
    "            # Заменяем пробелы на _ в многословных навыках\n",
    "            if len(skill_text.split()) > 1:\n",
    "                skill_text = skill_text.replace(' ', '_')\n",
    "            \n",
    "            # Добавляем дополнительные нормализации (опционально)\n",
    "            skill_text = skill_text.replace('ё', 'е')  # Приводим ё → е\n",
    "            pred_skills.append(skill_text)\n",
    "            \n",
    "        return pred_skills\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при обработке текста: {str(e)}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "09b58c49-9f72-4912-971b-feaf652448c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Расчет метрик\n",
    "def evaluate(golden_df):\n",
    "    results = []\n",
    "    all_true = []\n",
    "    all_pred = []\n",
    "    \n",
    "    for _, row in golden_df.iterrows():\n",
    "        true_skills = set(row['true_skills'])\n",
    "        pred_skills = set(predict_skills(row['job_requirements']))\n",
    "        \n",
    "        # Считаем TP, FP, FN\n",
    "        tp = len(true_skills & pred_skills)\n",
    "        fp = len(pred_skills - true_skills)\n",
    "        fn = len(true_skills - pred_skills)\n",
    "        \n",
    "        # Precision, Recall, F1\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        results.append({\n",
    "            'text': row['job_requirements'],\n",
    "            'true_skills': '; '.join(true_skills),\n",
    "            'pred_skills': '; '.join(pred_skills),\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'tp': tp,\n",
    "            'fp': fp,\n",
    "            'fn': fn\n",
    "        })\n",
    "        \n",
    "        all_true.extend(true_skills)\n",
    "        all_pred.extend(pred_skills)\n",
    "    \n",
    "    # Общие метрики\n",
    "    avg_precision = np.mean([r['precision'] for r in results])\n",
    "    avg_recall = np.mean([r['recall'] for r in results])\n",
    "    avg_f1 = np.mean([r['f1'] for r in results])\n",
    "    \n",
    "    # Микро-усредненные метрики\n",
    "    total_tp = sum(r['tp'] for r in results)\n",
    "    total_fp = sum(r['fp'] for r in results)\n",
    "    total_fn = sum(r['fn'] for r in results)\n",
    "    \n",
    "    micro_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n",
    "    micro_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n",
    "    micro_f1 = 2 * (micro_precision * micro_recall) / (micro_precision + micro_recall) if (micro_precision + micro_recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'results': pd.DataFrame(results),\n",
    "        'macro_metrics': {\n",
    "            'precision': avg_precision,\n",
    "            'recall': avg_recall,\n",
    "            'f1': avg_f1\n",
    "        },\n",
    "        'micro_metrics': {\n",
    "            'precision': micro_precision,\n",
    "            'recall': micro_recall,\n",
    "            'f1': micro_f1\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443060b3-5298-457c-bbd8-620fc340313f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = evaluate(golden_dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "ed217883-6080-423f-9419-710cadcd3d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Macro Metrics ===\n",
      "Precision: 0.789\n",
      "Recall: 0.729\n",
      "F1: 0.749\n",
      "\n",
      "=== Micro Metrics ===\n",
      "Precision: 0.819\n",
      "Recall: 0.720\n",
      "F1: 0.766\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Macro Metrics ===\")\n",
    "print(f\"Precision: {metrics['macro_metrics']['precision']:.3f}\")\n",
    "print(f\"Recall: {metrics['macro_metrics']['recall']:.3f}\")\n",
    "print(f\"F1: {metrics['macro_metrics']['f1']:.3f}\")\n",
    "\n",
    "print(\"\\n=== Micro Metrics ===\")\n",
    "print(f\"Precision: {metrics['micro_metrics']['precision']:.3f}\")\n",
    "print(f\"Recall: {metrics['micro_metrics']['recall']:.3f}\")\n",
    "print(f\"F1: {metrics['micro_metrics']['f1']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "491cd841-669f-4288-98fa-87450bd75974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 False Positives:\n",
      "ответственность: 50\n",
      "приемка_товара: 33\n",
      "1c: 10\n",
      ": 8\n",
      "консультирование: 7\n",
      "позитивный: 4\n",
      "выкладка_товара: 3\n",
      "внимательность: 2\n",
      "переговоры: 1\n",
      "высшее_образование: 1\n",
      "уверенный_пользователь_пк: 1\n",
      "амбициозный: 1\n",
      "вежливое_обслуживание_покупателей: 1\n",
      "исполнительность: 1\n",
      "эмпатия: 1\n",
      "желание_зарабатывать: 1\n",
      "выкладка_товаров: 1\n",
      "обслуживание_покупателей: 1\n",
      "контроль_сроков_годности: 1\n",
      "коммуникативные_навыки: 1\n",
      "пользователь_пк: 1\n",
      "оптовые_продажи: 1\n",
      "работоспособность: 1\n",
      "autocad: 1\n",
      "консультация_клиентов: 1\n",
      "преодоление_возражений: 1\n",
      "оформление_витрин: 1\n",
      "сопровождение_клиентов: 1\n",
      "продажа: 1\n",
      "компьютер: 1\n",
      "мерчендайзер: 1\n"
     ]
    }
   ],
   "source": [
    "# Анализ ошибок\n",
    "error_analysis = defaultdict(int)\n",
    "for _, row in metrics['results'].iterrows():\n",
    "    for skill in set(row['pred_skills'].split('; ')) - set(row['true_skills'].split('; ')):\n",
    "        error_analysis[skill] += 1\n",
    "\n",
    "print(\"\\nTop 10 False Positives:\")\n",
    "for skill, count in sorted(error_analysis.items(), key=lambda x: -x[1])[:]:\n",
    "    print(f\"{skill}: {count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Clean)",
   "language": "python",
   "name": "clean_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
